{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0dbb3a7d-9f05-4da0-9ebf-d518c5b2e9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import splitext, isfile, join\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, images_dir, mask_dir):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "       \n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(images_dir) if isfile(join(images_dir, file)) and not file.startswith('.')]\n",
    "        \n",
    "        if not self.ids:\n",
    "            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n",
    "     \n",
    "    \n",
    "    #change preprocess depending on how much tranformations you want\n",
    "    def img_preprocess(self, image):\n",
    "        #w, h = pil_img.size\n",
    "        \n",
    "        #image = TF.to_tensor(image)\n",
    "        image = np.asarray(image)\n",
    "        return image\n",
    "        \n",
    "    def mask_preprocess(self, mask):\n",
    "        #w, h = pil_img.size\n",
    "        #img = np.asarray(pil_img)\n",
    "        #mask = TF.to_tensor(mask)\n",
    "        mask = np.asarray(mask)\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #name from idx e.g. [0]\n",
    "        name = self.ids[idx]\n",
    "        \n",
    "        \n",
    "        #the one mask file from the given idx\n",
    "        mask_file = list(self.mask_dir.glob(name  + '.*'))\n",
    "        #same but for img_file\n",
    "        img_file = list(self.images_dir.glob(name + '.*'))\n",
    "\n",
    "        #makes the file an image\n",
    "        mask = load_image(mask_file[0])\n",
    "        img = load_image(img_file[0])\n",
    "\n",
    "        #\n",
    "        #image = np.load(img_path_input_patch).astype('float32')\n",
    "        #mask = np.load(img_path_tgt_patch).astype('float32')\n",
    "        img = self.img_preprocess(img)\n",
    "        mask = self.mask_preprocess(mask)\n",
    "\n",
    "        return {\n",
    "            'image': torch.as_tensor(img.copy()).float().contiguous(),\n",
    "            'mask': torch.as_tensor(mask.copy()).long().contiguous()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "#creating the dataset\n",
    "dataset = BasicDataset(\"/Users/zachderse//Documents/mice_training_data/images/\",\n",
    "                     \"/Users/zachderse//Documents/mice_training_data/labels/\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "343381fe-42a2-4da3-98b2-3847e52a7a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "n_val = int(len(dataset) * 0.8)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "#loading the dataset\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size = 10, num_workers=0)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, batch_size = 10, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d15681b7-4711-4d13-95d5-1ffc28c82d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "#train mode\n",
    "    model.train()\n",
    "    \n",
    "    #set loss and accuracy values to 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        images, true_masks = batch['image'], batch['mask']\n",
    "\n",
    "        # assert images.shape[1] == model.n_channels, \\\n",
    "        #     f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "        #     f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "        #     'the images are loaded correctly.'\n",
    "\n",
    "        images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "        true_masks = true_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "        with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "            masks_pred = model(images)\n",
    "            loss = criterion(masks_pred, true_masks)\n",
    "            loss += dice_loss(\n",
    "                F.softmax(masks_pred, dim=1).float(),\n",
    "                F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        grad_scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "        global_step += 1\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        return train_loss\n",
    "        #eval step, will try to not use\n",
    "        \n",
    "#         division_step = (n_train // (5 * batch_size))\n",
    "#                 if division_step > 0:\n",
    "#                     if global_step % division_step == 0:\n",
    "#                         histograms = {}\n",
    "#                         for tag, value in model.named_parameters():\n",
    "#                             tag = tag.replace('/', '.')\n",
    "#                             if not (torch.isinf(value) | torch.isnan(value)).any():\n",
    "#                                 histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "#                             if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
    "#                                 histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "#                         val_score = evaluate(model, val_loader, device, amp)\n",
    "#                         scheduler.step(val_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#         #old stuff\n",
    "#         y_pred = model(X)\n",
    "#         print(y)\n",
    "        \n",
    "        \n",
    "#         loss = loss_fn(y_pred, y)\n",
    "#         train_loss += loss.item() \n",
    "\n",
    "#         # Zero the gradient to remove previous optimizer data\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # back propagation\n",
    "#         loss.backward()\n",
    "\n",
    "#         #Step the optimizer\n",
    "#         optimizer.step()\n",
    "        \n",
    "\n",
    "#         # Calculate and accumulate accuracy metric across all batches\n",
    "#         y_pred_class = torch.argmax(torch.log_softmax(y_pred, dim=1), dim=1)\n",
    "#         train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "#     # Adjust metrics to get average loss and accuracy per batch \n",
    "#     train_loss = train_loss / len(dataloader)\n",
    "#     train_acc = train_acc / len(dataloader)\n",
    "#     return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "441dcbbf-5737-423d-911d-32219c192b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Switch to eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    num_val_batches = len(dataloader)\n",
    "    # Set loss and accuracy to zero\n",
    "    dice_score = 0\n",
    "    \n",
    "    \n",
    "    for batch in dataloader:\n",
    "            image, mask_true = batch['image'], batch['mask']\n",
    "\n",
    "            # move images and labels to correct device and type\n",
    "            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "            mask_true = mask_true.to(device=device, dtype=torch.long)\n",
    "\n",
    "            # predict the mask\n",
    "            mask_pred = model(image)\n",
    "\n",
    "            \n",
    "            assert mask_true.min() >= 0 and mask_true.max() < model.n_classes, 'True mask indices should be in [0, n_classes['\n",
    "            # convert to one-hot format\n",
    "            mask_true = F.one_hot(mask_true, model.n_classes).permute(0, 3, 1, 2).float()\n",
    "            mask_pred = F.one_hot(mask_pred.argmax(dim=1), model.n_classes).permute(0, 3, 1, 2).float()\n",
    "            # compute the Dice score, ignoring background\n",
    "            dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n",
    "\n",
    "    return dice_score\n",
    "    \n",
    "    \n",
    "#     # Turn on inference \n",
    "#     with torch.inference_mode():\n",
    "#         # Loop through DataLoader batches\n",
    "#         for batch, (X, y) in enumerate(dataloader):\n",
    "#             # Send data to target device\n",
    "#             X, y = X.to('cpu'), y.to('cpu')\n",
    "#             print(\"X \", X)\n",
    "#             print(\"y \", y)\n",
    "            \n",
    "            \n",
    "#             test_pred_logits = model(X)\n",
    "\n",
    "            \n",
    "#             loss = loss_fn(test_pred_logits, y)\n",
    "#             test_loss += loss.item()\n",
    "            \n",
    "#             # Calculate accuracy\n",
    "#             test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "#             test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "#     # Adjust metrics to get average loss and accuracy per batch \n",
    "#     test_loss = test_loss / len(dataloader)\n",
    "#     test_acc = test_acc / len(dataloader)\n",
    "#     return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "719671c6-b990-4935-bcfc-a76b44c9ca2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer,loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),epochs: int = 5):\n",
    "    #initialize loss and accuracy values\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []}\n",
    "    \n",
    "    #loop through epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "    \n",
    "    #print and update results\n",
    "        # print(\n",
    "        #         f\"Epoch: {epoch+1} | \"\n",
    "        #         f\"train_loss: {train_loss:.4f} | \"\n",
    "        #         f\"train_acc: {train_acc:.4f} | \"\n",
    "        #         f\"test_loss: {test_loss:.4f} | \"\n",
    "        #         f\"test_acc: {test_acc:.4f}\")\n",
    "        # results[\"train_loss\"].append(train_loss)\n",
    "        # results[\"train_acc\"].append(train_acc)\n",
    "        # results[\"test_loss\"].append(test_loss)\n",
    "        # results[\"test_acc\"].append(test_acc)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2c8efc19-ee38-429c-b422-2ec550f9889d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer,loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),epochs: int = 5):\n",
    "    #initialize loss and accuracy values\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model, dataloader=train_dataloader, loss_fn=loss_fn, optimizer=optimizer)\n",
    "        dice_score = test_step(model=model, dataloader=test_dataloader, loss_fn=loss_fn)\n",
    "        print(dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6df4f7fa-465f-4e46-8bd1-4c6639ef1e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08fb5ab8eb547159fce9c8773473606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "required rank 4 tensor to use channels_last format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train model_0 \u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model_results \u001b[38;5;241m=\u001b[39m train_model(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     17\u001b[0m                         train_dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     18\u001b[0m                         test_dataloader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     19\u001b[0m                         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     20\u001b[0m                         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, \n\u001b[1;32m     21\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# End the timer and print out how long it took\u001b[39;00m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[0;32mIn[143], line 4\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, train_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader, test_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader, optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,loss_fn: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#initialize loss and accuracy values\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m----> 4\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m train_step(model\u001b[38;5;241m=\u001b[39mmodel, dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m      5\u001b[0m         dice_score \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel, dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(dice_score)\n",
      "Cell \u001b[0;32mIn[140], line 21\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m images, true_masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# assert images.shape[1] == model.n_channels, \\\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     f'Network has been defined with {model.n_channels} input channels, ' \\\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     f'but loaded images have {images.shape[1]} channels. Please check that ' \\\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     'the images are loaded correctly.'\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mchannels_last)\n\u001b[1;32m     22\u001b[0m true_masks \u001b[38;5;241m=\u001b[39m true_masks\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, enabled\u001b[38;5;241m=\u001b[39mamp):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: required rank 4 tensor to use channels_last format"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_results = train_model(model=model, \n",
    "                        train_dataloader=train_loader,\n",
    "                        test_dataloader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=epochs)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c598a39-a724-431d-a04a-408d386d5d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): Encoder(\n",
       "    (encoding_blocks): ModuleList(\n",
       "      (0): EncodingBlock(\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): EncodingBlock(\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): EncodingBlock(\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): EncodingBlock(\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bottom_block): EncodingBlock(\n",
       "    (conv1): ConvolutionalBlock(\n",
       "      (conv_layer): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (activation_layer): ReLU()\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv2): ConvolutionalBlock(\n",
       "      (conv_layer): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (activation_layer): ReLU()\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoding_blocks): ModuleList(\n",
       "      (0): DecodingBlock(\n",
       "        (upsample): ConvTranspose2d(1024, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecodingBlock(\n",
       "        (upsample): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecodingBlock(\n",
       "        (upsample): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecodingBlock(\n",
       "        (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (conv1): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2): ConvolutionalBlock(\n",
       "          (conv_layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (activation_layer): ReLU()\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ConvolutionalBlock(\n",
       "    (conv_layer): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet import UNet\n",
    "device = \"cpu\"\n",
    "model = UNet()\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "model.to(device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
