{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c637e9-6959-47ba-b8da-8746b11f73fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae91b7b-67bd-4267-a845-5c8dd1c92716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####  DATASET IMPORT GOES HERE  ####\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from datasets import Dataset, DatasetDict, Image\n",
    "\n",
    "\n",
    "dir_path = \"\" # filepath here\n",
    "#get file names\n",
    "im_dir = os.listdir(dir_path)\n",
    "im_dir = [dir_path + s for s in im_dir]\n",
    "im_dir.sort()\n",
    "\n",
    "\n",
    "#split by 80% to 20%\n",
    "image_paths_train = im_dir[:int(len(im_dir)*0.8)]\n",
    "image_paths_validation = im_dir[-int(len(im_dir)*0.2):]\n",
    "\n",
    "\n",
    "\n",
    "#now for the labels\n",
    "dir_path = \"\" #put path here\n",
    "#get file names\n",
    "lab_dir = os.listdir(dir_path)\n",
    "lab_dir = [dir_path + s for s in lab_dir]\n",
    "lab_dir.sort()\n",
    "\n",
    "#split the same as the images\n",
    "label_paths_train = lab_dir[:int(len(lab_dir)*0.8)]\n",
    "label_paths_validation = lab_dir[-int(len(lab_dir)*0.2):]\n",
    "    \n",
    "#create dataset from the two filepaths\n",
    "def create_dataset(image_paths, label_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n",
    "                                \"label\": sorted(label_paths)})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "    return dataset\n",
    "\n",
    "#creating Dataset objects\n",
    "train_dataset = create_dataset(image_paths_train, label_paths_train)\n",
    "validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n",
    "\n",
    "#creating DatasetDict\n",
    "ds = DatasetDict({\n",
    "     \"train\": train_dataset,\n",
    "     \"validation\": validation_dataset,\n",
    "     }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c3374-ea92-4422-8e66-9ed841c7c216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "#creating labels where 0 = background, 1 = 1st mask, 2 = 2nd mask\n",
    "id2label = {0: 'BG', 1: 'label1', 2: 'label2'}\n",
    "with open('id2label.json', 'w') as fp:\n",
    "    json.dump(id2label, fp)\n",
    "    \n",
    "    \n",
    "train_ds = ds[\"train\"]\n",
    "valid_ds = ds[\"validation\"]\n",
    "\n",
    "#Check test and train set if data is properly set up\n",
    "valid_ds[0]\n",
    "\n",
    "\n",
    "#jitter to improve learning with this dataset\n",
    "jitter = transforms.ColorJitter(contrast=0.5, saturation=0.25)\n",
    "\n",
    "#dictionary for label id - done already with the custom dataset\n",
    "\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "\n",
    "\n",
    "#PREPROCESSING\n",
    "\n",
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint) #reduce_labels=True removed to include background class\n",
    "\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"label\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"label\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "train_ds.set_transform(train_transforms)\n",
    "valid_ds.set_transform(val_transforms)\n",
    "\n",
    "import evaluate\n",
    "#mean intersection over union for evaluation calculation\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a37e4-49a5-4ef8-b100-ebf74a6fb885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#finding the metrics for the model\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        print(type(logits), logits.shape)\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8b7b2-6124-49da-9441-4cf3c33a08cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer,  AutoModelForSemanticSegmentation\n",
    "#model for semantic segmentation\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab7f2b-0a14-416d-ae57-21541e5d0dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#various hyperparameters to input to the model when training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"/Users/zachderse/Documents\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=7, #was at 50\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    use_cpu = False\n",
    ")\n",
    "\n",
    "#trainer for the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d020dc2-0b55-4f3f-95a5-a56142670cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# checking the model visually through the validation set\n",
    "#open an example image from the validation set\n",
    "image = ds[\"validation\"][0][\"image\"]\n",
    "\n",
    "#using GPU or CPU depending on availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#encode the image\n",
    "encoding = image_processor(image, return_tensors=\"pt\")\n",
    "pixel_values = encoding.pixel_values.to(device)\n",
    "\n",
    "#print the image\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5617d-a5b6-4c03-aa2b-d331f986557a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use the trained model to generate a mask\n",
    "outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu() \n",
    "print(outputs[0].shape)\n",
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "print(pred_seg.shape)\n",
    "\n",
    "# make each label a separate color, and leave the background as no color\n",
    "def ade_palette():\n",
    "    return np.asarray([\n",
    "        [0, 0, 0],\n",
    "        [0, 250,0],        \n",
    "        [250,0,250]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7a681-36a6-48b1-bc97-716d0f38047a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### map ###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_seg == label, :] = color\n",
    "color_seg = color_seg[..., ::-1]  # convert to BGR\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336abf7e-ac30-4945-bb7e-e30ec5db9308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#using cv2 to remove very small labels through opening and closing\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10)) \n",
    "color_seg_alt = cv2.morphologyEx(color_seg, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)) \n",
    "color_seg_alt = cv2.morphologyEx(color_seg_alt, cv2.MORPH_CLOSE, kernel, iterations=4)\n",
    "\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg_alt * 0.5  # plot the image with the segmentation map\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
